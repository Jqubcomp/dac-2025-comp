<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<!-- saved from url=(0030)http://iccad-contest.org/2020/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta property="og:url" content="https://www.dac.com/">
  <meta charset="UTF-8">
  <meta property="og:image"
    content="https://www.dac.com/Portals/_default/skins/siteskin/images/DAC_62_Tag_Rev2.svg?v=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css"
    integrity="sha384-zCbKRCUGaJDkqS1kPbPd7TveP5iyJE0EjAuZQTgFLD2ylzuqKfdKlfG/eSrtxUkn" crossorigin="anonymous">
  <link rel="stylesheet" href="style.css">
  <!-- Scrollbar Custom CSS -->
  <link rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/malihu-custom-scrollbar-plugin/3.1.5/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <title>System Design Contest In DAC25</title>
</head>

<body>

  <div class="wrapper">
    <!-- Sidebar -->
    <nav id="sidebar">
      <div class="sidebar-header">
        <h3><a href="Contest.html">On-Device Multi-modal Generative AI for Science Contest at DAC 2025</a></h3>
      </div>

      <ul class="list-unstyled components">
        <li>
          <a href="index.html"><i class="fa fa-home" aria-hidden="true"></i> Home</a>
        </li>
        <li>
          <a href="Contest.html"><i class="fa fa-book" aria-hidden="true"></i> Contest</a>
        </li>
        <li>
          <a href="News.html"><i class="fa fa-info-circle" aria-hidden="true"></i> </i> News</a>
        </li>
        <li>
          <a href="FAQ.html"><i class="fa fa-question-circle" aria-hidden="true"></i> FAQ</a>
        </li>
        <li>
          <a href="Registration.html"><i class="fa fa-user-plus" aria-hidden="true"></i> Registration</a>
        </li>
        <li>
          <a href="Submission.html"><i class="fa fa-upload" aria-hidden="true"></i> Submission</a>
        </li>
        <li>
          <a href="Contact.html"><i class="fa fa-comments-o" aria-hidden="true"></i> Contact</a>
        </li>
      </ul>

    </nav>
    <div id="content">
      <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <!--           <div class="container-fluid">
            <button type="button" id="sidebarCollapse" class="btn btn-outline-secondary">
              <span class="navbar-toggler-icon"></span>
            </button>
            <ul class="navbar-nav">
              <li class="nav-item">
                <a href="http://ieee-ceda.org/">
                                      <img border="0" src="Figures/ESweek_2023.png" width="100" height="100"></a>
              </li>
              <li class="nav-item active" style="margin:auto">
                <a class="nav-link" href="problem.html"><h2 >Fair and Intelligent Embedded System Challenge </h2><span class="sr-only">(current)</span></a>
              </li>

             
            </ul>
          </div>  -->
        <div class="container-fluid">
          <button type="button" id="sidebarCollapse" class="btn btn-outline-secondary">
            <span class="navbar-toggler-icon"></span>
          </button>
          <a href="https://www.dac.com/"><img border="0" src="Figures/DAC_62_Square_name.png" width="100"
              height="100"></a>
          <div>
            <h2><a class="nav-link" href="/">On-Device Multi-modal Generative AI for Science Contest at DAC 2025<span
                  class="sr-only">(current)</span></a></h2>
          </div>

        </div>
      </nav>

      <hr style="width: 100%; height: 2px;">
      <div class="container">
        <h3>
          Contest
        </h3>
        <h5>
          1. Introduction
        </h5>
        <p>

          The breakthroughs in generative AI technology have dramatically transformed everyday life, exemplified by
          the emergence of ChatGPT and the recent Deepseek.
          In the realm of images, the advent of diffusion models has made high-quality image generation a reality.
          This has significant implications for the real world, as it allows for creating images in various styles
          tailored to human needs, with applications in production, education, work, and artistic creation.
          However, the significance of these advancements is somewhat limited when it comes to scientific data
          generation.
          Unlike natural images, generating standalone scientific data presents unique challenges, as such data
          typically serve specialized purposes and rely heavily on specific scientific contexts and applications.
          Moreover, the original diffusion models are too large to be placed on resource-limited devices for executions,
          while the scientific applications are commonly time-critical and require in-situ analyses and decision-making.
          To address these pressing issues, this competition takes a fundamental Full Waveform Inversion (FWI) task as a
          vehicle to demonstrate and explore the fundamental needs, major challenges, and potential solutions when AI
          for Science meets the resource-constrained embedded systems.
          In particular, we focus on data-driven Seismic FWI, a representative task in Geophysical applications.

        </p>
        <p>
          Traditional physics-based FWI: Traditional FWI methods aim to reconstruct subsurface velocity models by
          iteratively minimizing the difference between observed and simulated
          seismic data, typically using gradient-based optimization methods. The key challenge lies in solving the wave
          equation, which governs wave propagation through the Earth. While effective, these methods are computationally
          expensive and sensitive to factors such as the quality of the initial
          velocity model, noise in the data, and cycle-skipping issuesâ€”where the inversion algorithm converges to
          incorrect solutions due to poor starting models or insufficient low frequency data
          In recent years, machine
          learning approaches have been increasingly explored for
          FWI. Convolutional Neural Networks (CNNs) have shown
          promise in learning image-to-image mappings from seismic
          data to velocity models, bypassing the need for iterative
          solvers.
        </p>
        <p>
          While data-driven models offer great potential for portable, real-time, and detailed subsurface imaging, they
          have significant limitations.
          Unlike computer vision, the subsurface geophysics field is challenged by data scarcity, mainly due to a
          prevalent culture of non-sharing data.
          What's worse, in practical applications, the imbalance of data modalities presents a significant challenge.
          Velocity maps, which are more intuitive and understandable for humans, can be more easily simulated through
          various physical methods.
          In contrast, seismic data-critical for understanding subsurface structures is often more difficult and
          expensive to acquire. This imbalance results in an abundance of velocity maps, while the corresponding seismic
          data needed to create paired datasets remains limited.
          Therefore, efficiently generating paired multi-modal data is crucial for achieving accurate and comprehensive
          subsurface modeling, as it addresses the real-world scarcity of balanced, high-quality datasets.
        </p>

        <h5>
          2. Objective
        </h5>
        <p>
          The goal of the <em>On-Device Multi-modal Generative AI for Science Contest at DAC 2025</em> is to achieve the
          best performance in multi-modal (seismic wave and velocity maps) generation quality and and latency for the
          hardware platform from the FWI dataset.
        </p>
        <p>
          The participants are required to design and implement a working, open-source generative AI algorithm that can
          automatically generate seismic data and velocity maps simultaneously while being able to be deployed and run
          on the given platform [1].
          We will award prizes to the teams with top comprehensive performances in terms
          of multi-modal generation quality, and inference latency.
        </p>
        <h5>
          3. Data
        </h5>
        <p>
          The dataset for this contest is the <em>FWI dataset</em>, which is a collection of seismic data and velocity
          maps.
          The dataset is generated from the <em>Seismic FWI</em> simulation, which is a widely used method for
          subsurface imaging in geophysics.
          The dataset contains a total of XXX pairs of seismic data and velocity maps, with each pair representing a
          unique subsurface model.

        </p>

        <h5>
          4. Platform
        </h5>
        <p>
          Raspberry Pi [1] boards are tiny, incredibly versatile computers that have been put to an increasing number of
          practical, fun,
          and diverse uses by hobbyists. This exceptional flexibility has only been increased over the years by
          manufacturers coming out with
          a plethora of add-ons like sensors, touchscreens, wireless connectivity modules, and purpose-built cases. This
          latest main-line
          generation from the Raspberry Pi Project includes the Raspberry Pi 4 with 8GB of
          onboard RAM.
        </p>
        <p>
          The board has 2 \( \times \) micro-HDMI ports (up to 4kp60 supported), a 2-lane MIPI DSI display port, a
          2-lane MIPI CSI
          camera port, a Micro-SD card slot for loading the operating system, and data storage 5V DC via USB-C
          connector, 5V
          DC via GPIO header. 2.4 GHz and 5.0 GHz IEEE 802.11ac wireless, Bluetooth 5.0, BLE Gigabit Ethernet and for
          the memory.
        </p>
        <p>
          <a href="https://www.amazon.com/Raspberry-Pi-Computer-Suitable-Workstation/dp/B0899VXM8F/">
            Purchase Link (Amazon)<br>
            <img border="0" src="Figures/platform.png"><br>
          </a>
        </p>
        <h5>
          5. Scoring
        </h5>
        <p>
          We will evaluate the submitted algorithm with the scoring metric that evaluates the comprehensive performances
          in terms
          of detection performances and practical performances from macro, pairwise quality, and latency. It is defined
          as follows:
        </p>
        <p>
          For macro perspective:
        </p>
        <ul>
          <li>
            <p>
              FID Score: we will compute the FID (Frechet Inception Distance) score [1] of the generated seismic data
              and velocity maps
              with respect to the ground truth data. The FID score is a widely used metric for evaluating the quality of
              generated images,
              and it measures the distance between the distributions of the generated and real data in feature space. It
              is expected to be as low as possible, indicating that the generated data is similar to the real data.
            </p>
            <p> We will train a InceptionNet on the dataset, and then compute the FID score of the generated seismic
              data and velocity maps with respect to the ground truth data.</p>

            The normalized FID score will be calculated as follows:
            $$ FID_v = 1 - \frac{F_{vel} - F_{vmin}}{F_{vmax} - F_{vmin}} $$

            $$ FID_s = 1 - \frac{F_{seis} - F_{smin}}{F_{smax} - F_{smin}} $$
            where \( F_{seis} \) and \( F_{vel} \) represents the FID score of the generated seismic
              wave velocity map, and \( FID_v \) and \( FID_s \) means the normalized FID score of the generated seismic
              wave velocity map. 
              \( F_{vmin}\), \( F_{vmax}\), \( F_{smin}\), and \( F_{smax}\) TBD soon.
              <!-- \( F_{vmin}=0 \), \( F_{vmax}=100\), \( F_{smin}=0\), and \( F_{smax}=100\).<font color="red"> </font> -->

          </li>
        </ul>
        <p>
          For pairwise quality perspective:
        </p>
        <ul>
          <li>
            SSIM Score: we will compute the SSIM (Structural Similarity Index) score [2] by inputting the generated
            seismic data to a pre-trained InversionNet [3] to get the corresponding velocity map, and then compute the
            SSIM score.
            The SSIM score is a widely used metric for evaluating the quality of generated images, and it measures the
            similarity between two images in terms of luminance, contrast, and structure.
            It is expected to be as high as possible, indicating that the pairwise quality of the generated seismic data
            and velocity map is good.
            The SSIM score will be normalized by
            <p>
              $$ SSIM = \frac{S - S_{min}}{S_{max} - S_{min}} $$
            </p>
            \( S_{min}\) and \( S_{max}\) TBD soon.


          </li>
        </ul>
        <p>
          For latency perspective:
        </p>
        <ul>
          <li>
            Inference Latency: We will run inference 10 times to get the average latency L (in s) on Raspberry Pi 4.
            <!-- <img border="0" src="Figures/l_c.png" > -->
            The average inference latency L will be recorded The latency score will be normalized by
            <p>
              $$ L_n = 1 - \frac{L - L_{min}}{L_{max} - L_{min}} $$
            </p>
            <!-- where <font color="red"> L_min=0s, and L_max=100s.</font> -->
            \( L_{min}\) and \( L_{max}\) TBD soon.


          </li>
        </ul>
        <P>
          The final score will be calculated as follows:
        </p>
        <P>
          $$ Score = \frac{1}{3} \times (\frac{1}{2} \times (FID_{s} + FID_{v})) + \frac{1}{3} \times SSIM + \frac{1}{3} \times L_n $$
        </p>

        <h5>
          6. References
        </h5>
        <p>
          [1] Platform. https://www.raspberrypi.com/for-home/.

        <p>
          [2] Heusel, Martin, et al. "Gans trained by a two time-scale update rule converge to a local nash
          equilibrium." Advances in neural information processing systems 30 (2017).

        </p>
        <p>
          [3] Wang, Zhou, et al. "Image quality assessment: from error visibility to structural similarity." IEEE
          transactions on image processing 13.4 (2004): 600-612.
        </p>
        <p>
          [4] Wu, Yue, and Youzuo Lin. "InversionNet: An efficient and accurate data-driven full waveform inversion."
          IEEE Transactions on Computational Imaging 6 (2019): 419-433.
        </p>

      </div>
    </div>

  </div>
  <div class="card-footer text-muted text-center">
    Copyright &copy; On-Device Multi-modal Generative AI for Science Contest at DAC. All Rights Reserved.
  </div>

  <!-- jQuery CDN - Slim version (=without AJAX) -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <!-- Popper.JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js"
    integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ"
    crossorigin="anonymous"></script>
  <!-- Bootstrap JS -->
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/js/bootstrap.min.js"
    integrity="sha384-uefMccjFJAIv6A+rW+L4AHf99KvxDjWSu1z9VI8SKNVmz4sk7buKt/6v9KI65qnm"
    crossorigin="anonymous"></script>
  <!-- jQuery Custom Scroller CDN -->
  <script
    src="https://cdnjs.cloudflare.com/ajax/libs/malihu-custom-scrollbar-plugin/3.1.5/jquery.mCustomScrollbar.concat.min.js"></script>

  <script type="text/javascript" src="sidebar.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

</body>

</html>